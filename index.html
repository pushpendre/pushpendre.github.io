<html>
<head>
      <meta charset="UTF-8" />
      <title>Pushpendre Rastogi Homepage</title>
      <style> .menu {
            float:left;
            width:220px;
          }
          .mainContent {
            float:left;
            width:75%;
          }
          tr.spaceUnder>td {
              padding-bottom: 1em;
          }
      </style>
      <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/default.min.css">
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script>
</head>

<body><div class="menu"><img src="res/header.png" width="90%" /><a class="twitter-timeline" data-height="725" data-width="200" data-dnt="true" href="https://twitter.com/Pushpendre89?ref_src=twsrc%5Etfw">Twitter</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><div class="main-content"><header class="col-span">
<h1 class="title counter-skip">Pushpendre Rastogi</h1>
<h2 class="subtitle counter-skip">pushpendre at gmail</h2></header>
<h2>Introduction</h2><p>I joined the Dialog State Tracking in Amazon Alexa in April 2019. I completed my Ph.D. in Computer Science at <a href="http://www.clsp.jhu.edu">The Center For Language and Speech Processing, Johns Hopkins University</a>. My advisor was <a href="http://www.cs.jhu.edu/~vandurme/">Benjamin Van Durme</a>. I TA'd graduate courses on representation learning and machine learning for three semesters during my Phd studies, and I received the <a href="https://engineering.jhu.edu/excellence-teaching-awards/#tbs_nav_item_1">George Sommerman Graduate Teaching Assistant Award</a> with a cash award of $1000. I have reviewed for Transactions On Signal Processing-19, NEURIPS-19, ICML-19, ICLR-19, EMNLP-19, ACL-19, TPAMI-18, NeurIPS-18, KG4IR-18, EMNLP-18, ACL-18.</p>
<h2>Selected Publications</h2>
<p>See my <a href="https://scholar.google.com/citations?user=nqDASHMAAAAJ">google scholar profile</a> for a complete list of publications.</p> 
      <ul>
            <li>"Improving long distance slot carryover in spoken dialog systems".<a href="http://www.cs.jhu.edu/~tongfei/">Tongfei Chen</a>, Chetan Naik, Hua He, Pushpendre Rastogi, and Lambert Mathias. (2019) <a href="https://arxiv.org/abs/1906.01149">[arxiv]</a> <a style="background-color: rgb(255,255,0); color: rgb(255,0,0)" href="https://sites.google.com/view/nlp4convai/program">[bestpaper]</a></li>
            <li>"Scaling Multi-Domain Dialogue State Tracking via Query Reformulation".Pushpendre Rastogi, <a href="https://www.linkedin.com/in/arpit-gupta-77759719/">Arpit Gupta</a>, <a href="http://www.cs.jhu.edu/~tongfei/">Tongfei Chen</a>, and Lambert Mathias. (2019) <a href="https://arxiv.org/abs/1903.05164">[arxiv]</a> <a href="http://www.xuwei.io/2019/03/25/%E3%80%8Ascaling-multi-domain-dialogue-state-tracking-via-query-reformulation%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0">[chinese-translation-1]</a> <a href="https://www.facebook.com/pushpendre/posts/2555616624460105">[chinese-translation-2]</a> <a href="https://github.com/alexa/alexa-dataset-contextual-query-rewrite">[dataset]</a> <a style="background-color: rgb(255,255,0); color: rgb(255,0,0)" href="https://venturebeat.com/2019/06/06/amazons-ai-rewrites-voice-commands-in-natural-language-to-reduce-false-positives">[press-venturebeat]</a></li>
            <li>"A dataset for resolving referring expressions in spoken dialogue via contextual query rewrites (cqr)".Michael Regan, Pushpendre Rastogi, <a href="https://www.linkedin.com/in/arpit-gupta-77759719/">Arpit Gupta</a>, and Lambert Mathias. (2019) <a href="https://arxiv.org/1903.11783">[arxiv]</a> <a href="https://github.com/alexa/alexa-dataset-contextual-query-rewrite">[dataset]</a></li>
            <li>"Neural variational entity set expansion for automatically populated knowledge graphs".Pushpendre Rastogi, <a href="https://www.cs.jhu.edu/~apoliak1/">Adam Poliak</a>, <a href="https://www.ams.jhu.edu/~lyzinski/">Vince Lyzinski</a>, and <a href="http://www.cs.jhu.edu/~vandurme/">Benjamin Van Durme</a>. (2018) <a href="/res/nvse.bib">[bib]</a> <a href="https://doi.org/10.1007/s10791-018-9342-1">[doi]</a> <a href="https://github.com/se4u/nvse">[code]</a> <a href="https://youtu.be/sGO_wvuPIzM">[demo]</a><a href="https://github.com/se4u/nvse/blob/master/kg4ir_journal_tex/kg4irjournal.pdf">[pdf]</a></li>
            <li>"Efficient, compositional, order-sensitive n-gram embeddings".<a href="https://www.cs.jhu.edu/~apoliak1/">Adam Poliak</a>, Pushpendre Rastogi, M. Patrick Martin, and <a href="http://www.cs.jhu.edu/~vandurme/">Benjamin Van Durme</a>. (2017) <a href="http://www.aclweb.org/anthology/E17-2081">[pdf]</a> <a href="http://www.aclweb.org/anthology/E17-2081.bib">[bib]</a> <a href="https://github.com/azpoliak/eco">[code]</a> <a href="https://www.cs.jhu.edu/~apoliak1/files/posters/ECO--EACL-2017-poster.pdf">[poster]</a></li>
            <li>"Vertex nomination on the cold start knowledge graph".Pushpendre Rastogi, <a href="https://www.ams.jhu.edu/~lyzinski/">Vince Lyzinski</a>, and <a href="http://www.cs.jhu.edu/~vandurme/">Benjamin Van Durme</a>. (2017) <a href="/res/kbvntr.pdf">[pdf]</a> <a href="/res/kbvntr.bib">[bib]</a></li>
            <li>"Weighting finite-state transductions with neural context".Pushpendre Rastogi, <a href="https://ryancotterell.github.io/">Ryan Cotterell</a>, and <a href="http://www.cs.jhu.edu/~jason/">Jason Eisner</a>. (2016) <a href="http://www.aclweb.org/anthology/N16-1076.bib">[bib]</a> <a href="http://www.aclweb.org/anthology/N16-1076">[pdf]</a> <a href="/res/rastogi2016weighting.slides.pdf">[slides]</a> <a href="https://github.com/se4u/neural_wfst.git">[code]</a></li>
            <li>"Efficient implementation of enhanced adaptive simultaneous perturbation algorithms".Pushpendre Rastogi, <a href="https://www.linkedin.com/in/jingyizhu/">Jingyi Zhu</a>, and <a href="http://www.ams.jhu.edu/~spall/Personal/">James Spall</a>. (2016) <a href="/res/rastogi2016efficient.bib">[bib]</a> <a href="https://github.com/se4u/FASPSA">[code]</a> <a href="/res/rastogi2016efficient.pdf">[pdf]</a> <a href="https://github.com/facebookresearch/nevergrad/pull/16">[nevergrad]</a></li>
            <li>"Multiview lsa: Representation learning via generalized cca".Pushpendre Rastogi, <a href="http://www.cs.jhu.edu/~vandurme/">Benjamin Van Durme</a>, and <a href="http://www.cs.jhu.edu/~raman/">Raman Arora</a>. (2015) <a href="http://www.aclweb.org/anthology/N15-1058">[pdf]</a> <a href="https://zenodo.org/record/16710">[data]</a> <a href="https://github.com/se4u/mvlsa">[code]</a> <a href="http://www.aclweb.org/anthology/N15-1058.bib">[bib]</a> <a href="/mvlsa/mvlsa_poster.pdf">[poster]</a> <a href="/mvlsa/multiview-lsa-proofs-and-faq.html">[supplementary]</a></li>
            <li>"Stationarity condition for fractional sampling filters".Pushpendre Rastogi. (2011) <a href="/res/mtp.pdf">[pdf]</a></li></ul>
<h2>Education</h2><table><tbody><tr><td style="text-align: left;">Ph.D. and M.S. in Computer Science</td><td>Johns Hopkins University</td><td>2013-19</td><td>3.75/4.0</td></tr><tr><td colspan="4" style="text-align: left;">Thesis Topic: Representation Learning for Words and Entities. I presented new methods for unsupervised learning of word and entity embeddings from texts and knowledge bases.<br />Courses and Grades: Natural Language Processing (A), Machine Learning in Complex Domains (A), Stochastic Search & Optimization (B), Parallel Programming (A-), Principles of Programming Languages (A-), Combinatorial Optimization (A+), Introduction to Convexity (A-)</td></tr><tr><td style="text-align: left;">M.Tech. in Information and Communication Technology</td><td>IIT Delhi</td><td>2010-11</td><td>8.77/10</td></tr><tr><td style="text-align: left;">B.Tech. in Electrical Engg.</td><td>IIT Delhi</td><td>2006-10</td><td>8.86/10</td></tr></tbody></table>
      
<h2>Code Snippets and tricks</h2>
<!--  <details>
            <summary> XXX </summary>
            <p><pre><code> YYYY </code></pre></p>
      </details> -->
      <details>
            <summary> Interactive pool for running jobs on the side in python. </summary>
            <p><pre><code> class InteractivePool:
    def __init__(self, J):
        import time
        self.J = J
        self.tic = time.time()
        
    def done(self):
        return sum(1 for j in self.J if j.done()), len(self.J)
    
    def collect(self,R=None):
        R = R or {}
        print(len(R))
        for i, j in enumerate(self.J):
            if i not in R and j.done() and not j.cancelled():
                R[i] = j.result()
        print(len(R))
        return R
    
    def time(self):
        import datetime, time as time_module
        return str(datetime.timedelta(seconds=time_module.time() - self.tic))
    
    def wait(self, interval=600):                                
        import time                                              
        while True:                                              
            time.sleep(interval)                                 
            if sum(1 for j in self.J if j.done()) == len(self.J):
                break                                            
        return                                                   

from concurrent.futures import ProcessPoolExecutor    
sidejob = ProcessPoolExecutor(max_workers=4).submit
P = InteractivePool([sidejob(f, i) for i in range(80)]) </code></pre></p>
</details>

<details>
      <summary> Save spark dataframe to sparse scipy arrays </summary>
      <p><pre><code> from functools import partial
import pyspark.ml as pm
from typing import * 
from scipy.sparse import csr_matrix, vstack, lil_matrix, load_npz, save_npz
from pyspark import TaskContext
from tempfile import TemporaryDirectory
from glob import glob

def sparseVectorList_to_CSRMatrix(X: List[pm.linalg.SparseVector]) -> csr_matrix:
     """ Convert list of pyspark sparse vectors to a scipy CSR matrix that
     a standard sklearn function/lightgbm can consume.
     """
     M = lil_matrix((len(X), X[0].size), dtype=np.float)
     for i, x in enumerate(X):
         I = np.argsort(x.indices)
         M.rows[i] = x.indices[I]
         M.data[i] = x.values[I]
     return M.tocsr(copy=False)

 class RowToPredict(NamedTuple):
     "This class was created just to facilitate linting and type hinting."
     customer_id: str
     features: pm.linalg.SparseVector

def save_features_in_spark_as_sparsescipy_to_hdfs(
     hdfs_dir,
     row_gen: Iterable[RowToPredict]):
     C, Flist = [], []
     for e in row_gen:
         Flist.append(e.features)
         C.append(e.customer_id)
     F = sparseVectorList_to_CSRMatrix(Flist)
     pid = TaskContext().partitionId()
     # Ideally I will upload file directly to HDFS, but I don't know how to directly
     # write to HDFS. hdfscli didn't work for me. So work-around is to save to local
     # file on task node, then upload to HDFS with a subprocess call.
     with TemporaryDirectory() as tmpdirname:
         print('created temporary directory', tmpdirname)
         fname = f'{tmpdirname}/F.{pid}.npz'
         cname = f'{tmpdirname}/C.{pid}.pkl'
         with open(fname) as fh:
             save_npz(fh, F)
         with open(cname) as fh:
             pickle.dump(C, fh)
         subprocess.getstatusoutput('hadoop fs -put {fname} {cname} {hdfs_dir}')
     return 

# make sure that each partition has a reasonable number of rows so that we don't OOM.
npart = sdf.count() // 10000 
sdf.rdd.repartition(npart).foreachPartition(partial(
    save_features_in_spark_as_sparsescipy_to_hdfs, '/data/')

# After saving all the parts to hdfs, download the parts, and open them on master node.
subprocess.getstatusoutput('hadoop -copyToLocal /data/ /home/hadoop/')
L = glob('data/*.npz')
F = vstack([load_npz(e) for e in L])
C = [c for e in L for c in pickle.load(open(e))] </code></pre></p>
</details>
      
<details>
      <summary> How to hash check pip files </summary>
      <p><pre><code> 1. Install virtualenv                                         
1. Create workspace, download package.                        
1. Go where the requirements file is.                         
1. Create fresh empty environment and activate it             
1. install all requirements                                   
1. generate hashes for all installed packages                 
1. close the shell and create new one                         
1. Create fresh empty environment and activate it             
1. check that the new requirements file can be installed with 

pip install virtualenv  pip-tools
python3 -m venv env;  source env/bin/activate                                                                                                                                                        
 pip list > before; pip install -r requirements.txt; pip list > after                                                                                                                          
 pip-compile requirements.txt --generate-hashes # this overwrites the original file.                                                                                                           
 exit; bash                                                                                                                                                                                           
 python3 -m venv env2; source env2/bin/activate                                                                                                                                                       
 pip install --require-hashes -r requirements.txt
</code></pre></p>
</details>

<details>
      <summary> hdfs file system functionality exposed to python </summary>
      <p><pre><code> def hdfs_exists(path, flag='-e'):
    code, output = subprocess.getstatusoutput(f'hadoop fs -test {flag} {path}')
    if code != 0:
        print(output)
        return False
    else:
        return True

def copyFromLocal(src, dst):
    return subprocess.getstatusoutput(f'hadoop fs -copyFromLocal {src} {dst}')

def copyToLocal(src, dst):
    return subprocess.getstatusoutput(f'hadoop fs -copyToLocal {src} {dst}') </code></pre></p>
</details>

<details>
      <summary> Spark Setup </summary>
      <p><pre><code> def setup(RUNDATE='2020-07-16', spark_setting_file=None):
    """ Construct spark session, setup logger, and read YAML files
    from prime-ml-repo in production EMR clusters. After reading yaml files
    format the paths with dates.

    RUNDATE is a date string like this '2020-05-30'
    """
    assert re.match('\d{4}-\d{2}-\d{2}', RUNDATE)
    if spark_setting_file is None:
        spark_setting_file = StringIO("""scoring:
     spark.executor.memory: '20G'
     spark.executor.memoryOverhead: '4G'
     spark.executor.cores: 4
     spark.task.cpus: 1
     spark.yarn.am.memory: '2G'
     spark.serializer: 'org.apache.spark.serializer.KryoSerializer'
     spark.driver.maxResultSize: 0
     spark.executor.extraJavaOptions: '-XX:+UseG1GC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps'
     spark.kryoserializer.buffer.max: '512M'
     spark.cleaner.periodicGC.interval: '5min'
     spark.network.timeout: '600s'""")
    # Use logger to log everything to file and also to stderr.
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter(
        "%(asctime)s - %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S"
    )
    fh = logging.FileHandler("/home/hadoop/scoring_log_file.log")
    fh.setLevel(logging.INFO)
    fh.setFormatter(formatter)
    logger.addHandler(fh)
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(formatter)
    logger.addHandler(ch)
    logger.info(f"Initialize job parameters. {RUNDATE}")

    parameters = {}

    logger.info("Initialize spark settings and spark session.")
    spark = SparkSession.builder.appName("claire")
    for key, value in yaml.safe_load(spark_setting_file)["scoring"].items():
        logger.info(f'spark: {key}={value}')
        spark = spark.config(key, value)
    spark = spark.enableHiveSupport().getOrCreate()
    spark.sparkContext.setLogLevel(os.environ.get("SPARK_LOG_LEVEL", "DEBUG")) 
    try:
        spark.sparkContext.setCheckpointDir("hdfs:///checkpoint/spark/")
    except Exception as exception:
        warnings.warn("Unable to set spark checkpoint directory !")
    return spark, parameters, logger </code></pre></p>
</details>

<details>
      <summary> Common model inspections on binary classification test set</summary>
      <p><pre><code> def tabulate(label, proba, **kwargs):
    """ Compute common statistics on a binary classification problem
    given the true labels and the class probabilities.
    """
    assert proba.shape[1] == 2
    assert len(label) == len(proba)
    R = SimpleNamespace()
    R.accuracy = (label == proba.argmax(1)).mean()
    R.majority_rule_accuracy = max(1 - label.mean(), label.mean())
    R.log_loss = -np.log(np.select([label==0, label==1],
                                   [proba[:, 0], proba[:, 1]])).mean()
    fpr, tpr, thresholds = skm.roc_curve(label, proba[:, 1])
    R.roc_auc = skm.auc(fpr, tpr)
    try:
        idx = np.where(fpr < 0.05)[0].max()
        R.tp_at_fp_less_than_5_percent = tpr[idx]
        R.fp_at_fp_less_than_5_percent = fpr[idx]
        R.threshold_at_fp_less_than_5_percent = thresholds[idx]
    except ValueError as e:
        print(e)
        pass
    precision, recall, thresholds = skm.precision_recall_curve(label, proba[:, 1])
    R.prauc = skm.auc(recall, precision)
    R.precision = precision
    R.recall = recall
    R.thresholds = thresholds
    idx = np.where(precision > 0.9)[0].min()
    R.smallest_precision_greater_than_90pct = precision[idx]
    R.recall_at_precision_90pct = recall[idx]
    R.threshold_at_precision_90pct = thresholds[idx]

    idx = np.where(precision > 0.5)[0].min()
    R.smallest_precision_greater_than_50pct = precision[idx]
    R.recall_at_precision_50pct = recall[idx]
    R.threshold_at_precision_50pct = thresholds[idx]

    R = R.__dict__
    R.update(kwargs)
    return R </code></pre></p>
</details>

<details>
      <summary> Boilerplate for configuring logger in python </summary>
      <p><pre><code> import logging
def setup_logger(file_path=None):
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter(
        "%(asctime)s - %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S"
    )

    if all(not isinstance(e, logging.FileHandler) 
           for e in logger.handlers):
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)
        ch.setFormatter(formatter)
        logger.addHandler(ch)
        logger.info(f"Initialized logger with StreamHandler")
    
    if file_path and all(not isinstance(e, logging.FileHandler)
                         for e in logger.handlers):
        fh = logging.FileHandler(file_path, 'a')
        fh.setLevel(logging.INFO)
        fh.setFormatter(formatter)
        logger.addHandler(fh)
        logger.info(f"Initialized logger with FileHandler({file_path})")

    return </code></pre></p>
</details>

<details>
      <summary> Java - Python/Numpy Fast Copy-Free Exchange </summary>
      <p><pre><code> /* Java */

        short a = 3; // 2
        long b = 5; // 8
        float c = (float)7.0; // 4
        ByteBuffer bb = ByteBuffer.allocate(14);
        bb.order(ByteOrder.LITTLE_ENDIAN);
        bb.putShort(a);
        pp(bb.position()); // 2
        bb.putLong(b);
        pp(bb.position()); // 10
        bb.putFloat(c);
        pp(bb.position()); // 14
        bb.position(0); // crucial.
        try(RandomAccessFile f = new RandomAccessFile("/tmp/tmp.dat", "rw");
            FileChannel fc = (f).getChannel();) {
                pp(bb.order().toString());
                pp(fc.write(bb));
        }


        ## Python

        from mmap import mmap, PROT_READ
        import os
        import numpy as np
        import sys
        assert sys.byteorder == 'little'
        fd = os.open('/tmp/tmp.dat', os.O_RDONLY)
        buf = mmap(fd, 14, prot=PROT_READ)
        # L = little endian.
        arr1 = np.frombuffer(buf, dtype=np.dtype('int16').newbyteorder('L'), count=1, offset=0)
        arr2 = np.frombuffer(buf, dtype=np.dtype('int64').newbyteorder('L'), count=1, offset=2)
        arr3 = np.frombuffer(buf, dtype=np.dtype('float32').newbyteorder('L'), count=1, offset=10)
        arr1, arr2, arr3 </code></pre></p>
</details>

<details>
      <summary> Java - Simple printing function. </summary>
      <p><pre><code> static void pp(Object format, Object... args) {
        System.out.printf(format.toString(), args);
        System.out.println();
    } </code></pre></p>
</details>

<details>
      <summary> Json encode numpy objects </summary>
      <p><pre><code> class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif type(obj).__module__ == 'numpy':
            if type(obj).__name__.startswith('float'):
                return float(obj)
            elif type(obj).__name__.startswith('int'):
                return int(obj)
            else:
                return bool(obj)
        return json.JSONEncoder.default(self, obj) </code></pre></p>
</details>

<details>
      <summary> List busy EMR machines and the total reserved machines </summary>
      <p><pre><code> aws emr list-clusters --region us-east-1 --active  > /tmp/tmp1
jq '.Clusters|.[]|.Id' /tmp/tmp1 -r | xargs -n 1 -I % sh -c 'aws emr list-instances --cluster-id % --instance-states RUNNING --region us-east-1' > /tmp/tmp2
jq '.Instances | .[] | .InstanceType' /tmp/tmp2 -r | sort | uniq -c
aws ec2 describe-reserved-instances --region us-east-1 | jq '.ReservedInstances | .[] | [.InstanceType, .InstanceCount, .State]' -c -r | fgrep -v retired | sort </code></pre></p>
</details>

<details>
      <summary> Excel VBA functions for testing significance of binomial A/B tests. Two-sided ZTest Pvalue from ratios and counts </summary>
      <p><pre><code> 'Two-sided ZTest Pvalue from counts
Public Function CountsZTest(count1, nob1, count2, nob2)
    CountsZTest = RatioZTest(count1 * 1# / nob1, nob1, count2 * 1# / nob2, nob2)
End Function
Public Function RatioZTest(p1, nob1, p2, nob2)
    diff = p1 - p2
    p_pooled = (p1 * nob1 + p2 * nob2) * 1# / (nob1 + nob2)
    nobs_2xhm = 1# / nob1 + 1# / nob2
    var1 = p_pooled * (1 - p_pooled) * nobs_2xhm
    std_diff = Sqr(var1)
    RatioZTest = Application.WorksheetFunction.Norm_S_Dist(-Abs(diff / std_diff), True) * 2
End Function </code></pre></p>
</details>

<!-- <details>
      <summary> XXX </summary>
      <p><pre><code> YYYY </code></pre></p>
</details>
 -->

      
<h2>Technical Notes</h2>
<!-- All notes are also hosted in <a href="https://www.dropbox.com/sh/u5e7eo8a5i4jyoz/AABaA_mTNow4kmtv7kWK5faAa?dl=0">a dropbox folder</a>. -->
<table>
  <tbody>
    <tr><td>
          <a href="res/note1.html">Note 1</a>: <a href="res/note1.pdf">[PDF]</a>Describes how the variance of an AB test can be reduced in the special case when we are comparing two policies with the same small-finite action space. 
          <details>
            <summary>हिंदी विवरण</summary>
            <p>हम दो विधियों/treatments के बीच में कितना फर्क है ये पता करना चाहते है। साधारण तरीका होगा AB testing/ randomized control trials जिसमें की हम randomly/बेतरतीब तरिके से आधे लोगो को विधि A आवंटित करते है और आधे को विधि B प्रदान करते है । उसके बाद दोनों दल में औसत फर्क का अंतर हम पता करते है । ये सबसे आसान पद्धति हैं  और   मानलो की इस पद्धति को इस्तेमाल करने पर हमे 10000 लोगो पे परीक्षण करना पड़ेगा ताकि हम 10% का फर्क दोनो दलों के बीच मे पता कर पाए। जो pdf मैंने भेजी है वो एक विशेष स्थिति का विश्लेषण प्रशेष करती है जो की 25%  कम sample इस्तेमाल करती है। ofcourse ये कोई नई तकनीक नही है सिर्फ मैने अपनी समझ के लिये लिखी है।</p>
          </details>
        </td>
    </tr>
    <tr><td> <a href="res/note2.mp4">Note 2</a> A video tutorial about the difference between PnL and Cashflow, and how a company can have positive cash flow but still make loss, without raising debt. (you may need to download the video and play with VLC)</td></tr>
    <tr><td> <a href="res/note3.pdf">Note 3</a> [WIP] A visual proof of the UCB algorithm. </td></tr>
    <tr><td> <a href="res/graphical_summary_of_elements_of_information_theory.pdf">Note 4</a> A visual summary of the inequalities govening Entropy, Cross Entropy, Joint Entropy, KL Divergence, and Mutual Information including the Data Processing Inequality.</td></tr>    
  </tbody>
</table>
<h2>Links</h2>
<p><a href="https://stackexchange.com/users/257045/pushpendre">
      <img src="https://stackexchange.com/users/flair/257045.png" 
           width="208" height="58" 
           alt="profile for Pushpendre on Stack Exchange" 
           title="profile for Pushpendre on Stack Exchange" />
      </a>
</p>
<p><a href="https://github.com/se4u">
      <svg width="5%" height="5%" viewBox="0 0 20 15">
            <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
      </svg>
      <h4>github</h4>
      </a>
</p>
<p><a href="https://www.linkedin.com/in/pushpendre/" >
      <img src="https://upload.wikimedia.org/wikipedia/commons/0/01/LinkedIn_Logo.svg" width="100" height="25"></img>
   </a>
</p>
<p><a href="https://github.com/pushpendre/pushpendre.github.io/blob/master/index.html">Edit this page</a></p>
<p><a href="https://github.com/pushpendre/pushpendre.github.io/deployments/activity_log?environment=github-pages">See deployment status</a></p>

</div>
</body>
</html>
